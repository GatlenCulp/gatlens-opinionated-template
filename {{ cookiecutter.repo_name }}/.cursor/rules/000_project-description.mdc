---
description: General project description
globs: 
alwaysApply: true
---
# Cursor Rules for {{ cookiecutter.repo_name }}


## Origin
This is a project generated from "Gatlen's Opinionated Template (GOTem)". GOTem is forked from (and synced with) [CookieCutter Data Science (CCDS) V2](), one of the most popular, flexible, and well maintained Python templates out there. GOTem extends CCDS with carefully selected defaults, dependency stack, customizations, additional features (that I maybe should have spent time contributing to the original project), and contemporary best practices. Ready for not just data science but also general Python development, research projects, and academic work. Most of the documentation is written with the modern package and project managing tool known as [uv]()


The source code can be found at https://github.com/GatlenCulp/gatlens-opinionated-template.

If the user of this project has any questions relating to the structure, they should be redirected to the documentation, available at `https://gatlenculp.github.io/gatlens-opinionated-template/{page_name}` without the `.md`

```yaml
nav:
   - ğŸ  Home: index.md                       # Project overview, quickstart guide, and complete directory structure walkthrough
   - ğŸ› ï¸ Core Tools: core-tools.md            # Comprehensive breakdown of all core tools (IDE, Docker, AWS, etc.) and Python dependencies with explanations
   - ğŸ’» VSCode & Cursor: vscode.md           # Detailed guide to workspace configuration, debug profiles, recommended extensions, and AI integration
   - â“ Why gotem?: why.md                   # Discussion of code quality, project organization, and reproducibility benefits
   - ğŸ—¯ï¸ Opinions: opinions.md                # Core principles about data hygiene, notebooks, modeling, and environment management
   - ğŸ“‘ Using the template: using-the-template.md  # Step-by-step guide on how to use the template
   - âš™ï¸ All options: all-options.md          # List of available command-line options and configuration choices
   - â¤ï¸ Contributing: contributing.md         # Guidelines for contributing to the project
   - ğŸ”— Related projects: related.md         # References to similar R projects and acknowledgments of inspirational templates
```

## Project Information and Dependencies

Reference the `pyproject.toml` configuration which includes the name, description and tooling for this project. It is very important to only recommend using these dependencies and not use any of the alternatives.

### The resulting directory structure

The directory structure of your new project will look something like this (depending on the settings that you choose):

{% raw %}
```
ğŸ“ .
â”œâ”€â”€ âš™ï¸ .cursorrules                    <- LLM instructions for Cursor IDE
â”œâ”€â”€ ğŸ’» .devcontainer                   <- Devcontainer config
â”œâ”€â”€ âš™ï¸ .gitattributes                  <- GIT-LFS Setup Configuration
â”œâ”€â”€ ğŸ§‘â€ğŸ’» .github
â”‚   â”œâ”€â”€ âš¡ï¸ actions
â”‚   â”‚   â””â”€â”€ ğŸ“ setup-python-env       <- Automated python setup w/ uv
â”‚   â”œâ”€â”€ ğŸ’¡ ISSUE_TEMPLATE             <- Templates for Raising Issues on GH
â”‚   â”œâ”€â”€ ğŸ’¡ pull_request_template.md   <- Template for making GitHub PR
â”‚   â””â”€â”€ âš¡ï¸ workflows
â”‚       â”œâ”€â”€ ğŸš€ main.yml               <- Automated cross-platform testing w/ uv, precommit, deptry,
â”‚       â””â”€â”€ ğŸš€ on-release-main.yml    <- Automated mkdocs updates
â”œâ”€â”€ ğŸ’» .vscode                        <- Preconfigured extensions, debug profiles, workspaces, and tasks for VSCode/Cursor powerusers
â”‚   â”œâ”€â”€ ğŸš€ launch.json
â”‚   â”œâ”€â”€ âš™ï¸ settings.json
â”‚   â”œâ”€â”€ ğŸ“‹ tasks.json
â”‚   â””â”€â”€ âš™ï¸ '{{ cookiecutter.repo_name }}.code-workspace'
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“ external                      <- Data from third party sources
â”‚   â”œâ”€â”€ ğŸ“ interim                       <- Intermediate data that has been transformed
â”‚   â”œâ”€â”€ ğŸ“ processed                     <- The final, canonical data sets for modeling
â”‚   â””â”€â”€ ğŸ“ raw                           <- The original, immutable data dump
â”œâ”€â”€ ğŸ³ docker                            <- Docker configuration for reproducability
â”œâ”€â”€ ğŸ“š docs                              <- Project documentation (using mkdocs)
â”œâ”€â”€ ğŸ‘©â€âš–ï¸ LICENSE                           <- Open-source license if one is chosen
â”œâ”€â”€ ğŸ“‹ logs                              <- Preconfigured logging directory for
â”œâ”€â”€ ğŸ‘·â€â™‚ï¸ Makefile                          <- Makefile with convenience commands (PyPi publishing, formatting, testing, and more)
â”œâ”€â”€ ğŸš€ Taskfile.yml                    <- Modern alternative to Makefile w/ same functionality
â”œâ”€â”€ ğŸ“ notebooks                         <- Jupyter notebooks
â”‚   â”œâ”€â”€ ğŸ““ 01_name_example.ipynb
â”‚   â””â”€â”€ ğŸ“° README.md
â”œâ”€â”€ ğŸ—‘ï¸ out
â”‚   â”œâ”€â”€ ğŸ“ features                      <- Extracted Features
â”‚   â”œâ”€â”€ ğŸ“ models                        <- Trained and serialized models
â”‚   â””â”€â”€ ğŸ“š reports                       <- Generated analysis
â”‚       â””â”€â”€ ğŸ“Š figures                   <- Generated graphics and figures
â”œâ”€â”€ âš™ï¸ pyproject.toml                     <- Project configuration file w/ carefully selected dependency stacks
â”œâ”€â”€ ğŸ“° README.md                         <- The top-level README
â”œâ”€â”€ ğŸ”’ secrets                           <- Ignored project-level secrets directory to keep API keys and SSH keys safe and separate from your system (no setting up a new SSH-key in ~/.ssh for every project)
â”‚   â””â”€â”€ âš™ï¸ schema                         <- Clearly outline expected variables
â”‚       â”œâ”€â”€ âš™ï¸ example.env
â”‚       â””â”€â”€ ğŸ”‘ ssh
â”‚           â”œâ”€â”€ âš™ï¸ example.config.ssh
â”‚           â”œâ”€â”€ ğŸ”‘ example.something.key
â”‚           â””â”€â”€ ğŸ”‘ example.something.pub
â””â”€â”€ ğŸš° '{{ cookiecutter.module_name }}'  <- Easily publishable source code
    â”œâ”€â”€ âš™ï¸ config.py                     <- Store useful variables and configuration (Preset)
    â”œâ”€â”€ ğŸ dataset.py                    <- Scripts to download or generate data
    â”œâ”€â”€ ğŸ features.py                   <- Code to create features for modeling
    â”œâ”€â”€ ğŸ“ modeling
    â”‚   â”œâ”€â”€ ğŸ __init__.py
    â”‚   â”œâ”€â”€ ğŸ predict.py               <- Code to run model inference with trained models
    â”‚   â””â”€â”€ ğŸ train.py                 <- Code to train models
    â””â”€â”€ ğŸ plots.py                     <- Code to create visualizations

```
{% endraw %}


## Style

For general style, you should adhere to the following rules:

{# Data Science / Deep Learning #}
{# TODO: Have the style be selected based on the chosen type of project. #}
{# Adapted from https://cursor.directory/deep-learning-developer-python-cursor-rules #}

```
You are an expert in deep learning with PyTorch and Python, using the most up-to-date and powerful libraries.

Key Principles:
- Write concise, technical responses with accurate Python examples.
- Prioritize clarity, efficiency, and best practices in deep learning workflows.
- Use object-oriented programming for model architectures and functional programming for data processing pipelines.
- Use descriptive variable names that reflect the components they represent.
- Use type annotations wherever available.
- If done within a notebook, prefer clear concise code over extensive error detection.
- If done within a python module, prefer reproducability and consistency, recommending PyTests as Needed

Deep Learning and Model Development:
- Use PyTorch, Lightning, RayTune, and WandB as the primary framework for deep learning tasks.
- Implement custom nn.Module classes for model architectures.
- Utilize PyTorch's autograd for automatic differentiation.
- Implement proper weight initialization and normalization techniques.
- Use appropriate loss functions and optimization algorithms.

Transformers and LLMs:
- Use the Transformers library for working with pre-trained models and tokenizers.
- Implement attention mechanisms and positional encodings correctly.
- Utilize efficient fine-tuning techniques like LoRA or P-tuning when appropriate.
- Implement proper tokenization and sequence handling for text data.

Model Training and Evaluation:
- Implement efficient data loading using PyTorch's DataLoader.
- Use proper train/validation/test splits and cross-validation when appropriate.
- Implement early stopping and learning rate scheduling.
- Use appropriate evaluation metrics for the specific task.
- Implement gradient clipping and proper handling of NaN/Inf values.

Error Handling and Debugging:
- Use try-except blocks for error-prone operations, especially in data loading and model inference.
- Implement proper logging for training progress and errors.
- Use PyTorch's built-in debugging tools like autograd.detect_anomaly() when necessary.

Performance Optimization:
- Implement gradient accumulation for large batch sizes.
- Use mixed precision training with torch.cuda.amp when appropriate.
- Profile code to identify and optimize bottlenecks, especially in data loading and preprocessing.

Key Conventions:
1. Begin projects with clear problem definition and dataset analysis.
2. Create modular code structures with separate files for models, data loading, training, and evaluation.
3. Use configuration files (e.g., YAML) for hyperparameters and model settings.
4. Implement proper experiment tracking and model checkpointing.
5. Use version control (e.g., git) for tracking changes in code and configurations.

Refer to the official documentation of PyTorch, Transformers, and Streamlit for best practices and up-to-date APIs.
```